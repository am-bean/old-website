@misc{beanLargeLanguageModels2023,
  title = {Large Language Models - {{Written}} Evidence},
  author = {Bean, Andrew M. and Kirk, Hannah Rose and M{\"o}kander, Jakob and Osborne, Cailean and Roberts, Huw and Ziosi, Marta},
  year = {2023},
  month = oct,
  number = {LLM0074},
  publisher = {{UK Parliament}},
  urldate = {2023-10-27},
  langid = {english},
}

@misc{khandelwalCasteistNotRacist2023,
  title = {Casteist but {{Not Racist}}? {{Quantifying Disparities}} in {{Large Language Model Bias}} between {{India}} and the {{West}}},
  shorttitle = {Casteist but {{Not Racist}}?},
  author = {Khandelwal, Khyati and Tonneau, Manuel and Bean, Andrew M. and Kirk, Hannah Rose and Hale, Scott A.},
  year = {2023},
  month = sep,
  number = {arXiv:2309.08573},
  eprint = {2309.08573},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.08573},
  urldate = {2023-10-19},
  abstract = {Large Language Models (LLMs), now used daily by millions of users, can encode societal biases, exposing their users to representational harms. A large body of scholarship on LLM bias exists but it predominantly adopts a Western-centric frame and attends comparatively less to bias levels and potential harms in the Global South. In this paper, we quantify stereotypical bias in popular LLMs according to an Indian-centric frame and compare bias levels between the Indian and Western contexts. To do this, we develop a novel dataset which we call Indian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and anti-stereotypical examples for caste and religion contexts. We find that the majority of LLMs tested are strongly biased towards stereotypes in the Indian context, especially as compared to the Western context. We finally investigate Instruction Prompting as a simple intervention to mitigate such bias and find that it significantly reduces both stereotypical and anti-stereotypical biases in the majority of cases for GPT-3.5. The findings of this work highlight the need for including more diverse voices when evaluating LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
}

@misc{kirkPresentBetterFuture2023,
  title = {The {{Past}}, {{Present}} and {{Better Future}} of {{Feedback Learning}} in {{Large Language Models}} for {{Subjective Human Preferences}} and {{Values}}},
  author = {Kirk, Hannah Rose and Bean, Andrew M. and Vidgen, Bertie and R{\"o}ttger, Paul and Hale, Scott A.},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07629},
  eprint = {2310.07629},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.07629},
  urldate = {2023-10-19},
  abstract = {Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories.First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
}

@misc{korgulExploringLandscapeLarge2023,
  title = {Exploring the {{Landscape}} of {{Large Language Models In Medical Question Answering}}: {{Observations}} and {{Open Questions}}},
  shorttitle = {Exploring the {{Landscape}} of {{Large Language Models In Medical Question Answering}}},
  author = {Korgul, Karolina and Bean, Andrew M. and Krones, Felix and McCraith, Robert and Mahdi, Adam},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07225},
  eprint = {2310.07225},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.07225},
  urldate = {2023-10-19},
  abstract = {Large Language Models (LLMs) have shown promise in medical question answering by achieving passing scores in standardised exams and have been suggested as tools for supporting healthcare workers. Deploying LLMs into such a high-risk context requires a clear understanding of the limitations of these models. With the rapid development and release of new LLMs, it is especially valuable to identify patterns which exist across models and may, therefore, continue to appear in newer versions. In this paper, we evaluate a wide range of popular LLMs on their knowledge of medical questions in order to better understand their properties as a group. From this comparison, we provide preliminary observations and raise open questions for further research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
}
